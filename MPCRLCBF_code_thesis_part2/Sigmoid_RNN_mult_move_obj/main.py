import multiprocessing
import os
import copy
import optuna
import numpy as np
import casadi as cs

from config import (
    SAMPLING_TIME,
    SEED,
    NUM_STATES,
    NUM_INPUTS,
    CONSTRAINTS_X,
    CONSTRAINTS_U,
)
from Classes import env, RLclass, RNN
from Functions import (
    run_simulation,
    run_simulation_randomMPC,
    generate_experiment_notes,
    objective,
    save_best_results
)


def main():
    """
    Main for raining a CBF RNN with MPC in a moving-obstacle environment.
    """
    
    # ─── Experiment variables ────────────────────────────────────────────
    dt = SAMPLING_TIME
    seed = SEED

    # Noise / exploration schedule
    initial_noise_scale = 10
    noise_variance = 5
    decay_at_end = 0.01
    
    num_episodes = 3000
    episode_update_freq = 10  # frequency of updates (e.g. update every 10 episodes)
    decay_rate = 1 - np.power(decay_at_end, 1 / (num_episodes / episode_update_freq))
    print(f"Computed noise decay_rate: {decay_rate:.4f}")

    # RL hyper-parameters
    alpha = 8e-3       # initial learning rate
    gamma = 0.95       # discount factor
    slack_penalty = 1e3  # penalty on slack variables in CBF constraints
    
    # Learning rate scheduler
    # patience = number of epochs with no improvement after which learning rate will be reduced
    patience = 10_000 
    lr_decay = 0.1     # factor to shrink the learning rate with after patience is reached

    # Episode / MPC specs
    episode_duration = 150
    mpc_horizon = 5
    replay_buffer_size = episode_duration * episode_update_freq  # buffer holding number of episodes (e.g. hold 10 episodes)
    
    #name of folder where the experiment is saved
    experiment_folder = "RNN_mult_move_obj_experiment_118"
    
    #check if file exists already, if yes raise an exception
    # if os.path.exists(experiment_folder):
    #     raise FileExistsError(f"Experiment folder '{experiment_folder}' already exists. Please choose a different name.")
    
    
    # ──Linear dynamics and MPC parameters───────────────────────────────────
    params_init = {
        "A": cs.DM([
            [1, 0, dt, 0],
            [0, 1, 0, dt],
            [0, 0, 1, 0],
            [0, 0, 0, 1],
        ]),
        "B": cs.DM([
            [0.5 * dt**2, 0],
            [0, 0.5 * dt**2],
            [dt, 0],
            [0, dt],
        ]),
        "b": cs.DM([0, 0, 0, 0]),
        "V0": cs.DM(0.0),
        "P": 100 * np.eye(NUM_STATES),
        "Q": 10 * np.eye(NUM_STATES),
        "R": np.eye(NUM_INPUTS),
    }
    
     # ─── Obstacle configuration ──────────────────────────────────────────────
    positions = [(-2.0, -1.5), (-3.0, -3.0)]
    radii     = [0.75, 0.75]
    modes     = ["step_bounce", "step_bounce"]
    mode_params = [
        {"bounds": (-4.0,  0.0), "speed": 2.3, "dir":  1},
        {"bounds": (-4.0,  1.0), "speed": 2.0, "dir": -1},
    ]
    
    # ─── Build & initialize RNN CBF ───────────────────────────────────────────

    input_dim = NUM_STATES + len(positions) + 2*len(positions) #x+h(x)+ (obs_positions_x + obs_positions_y)
    hidden_dims = [16, 16]
    output_dim = len(positions)
    layers_list = [input_dim] + hidden_dims + [output_dim]
    print("RNN layers:", layers_list)

    rnn = RNN(layers_list, positions, radii, mpc_horizon, mode_params)
    flat_rnn_params, _, _, _ = rnn.initialize_parameters()
    params_init["rnn_params"] = flat_rnn_params

    # keep a copy of the original parameters for later logging
    params_before = params_init.copy()
    
    
    # ─── The Learning  ─────────────────────────────────────
    
    # run simulation of random MPC to see how the system behaves under initial random noise
    
    run_simulation_randomMPC(
        params_init,
        env,
        experiment_folder,
        episode_duration,
        layers_list,
        initial_noise_scale,
        noise_variance,
        mpc_horizon,
        positions,
        radii,
        modes,
        copy.deepcopy(mode_params),
        slack_penalty,
    )
    
    # run simulation to get the initial policy before training
    stage_cost_before = run_simulation(
        params_init,
        env,
        experiment_folder,
        episode_duration,
        layers_list,
        after_updates=False,
        horizon=mpc_horizon,
        positions=positions,
        radii=radii,
        modes=modes,
        mode_params=copy.deepcopy(mode_params),
        slack_penalty_eval=slack_penalty,
    )
    
    # use RL to train the RNN CBF with MPC
    
    rl_agent = RLclass(
        params_init,
        seed,
        alpha,
        gamma,
        decay_rate,
        layers_list,
        initial_noise_scale,
        noise_variance,
        patience,
        lr_decay,
        mpc_horizon,
        positions,
        radii,
        modes,
        copy.deepcopy(mode_params),
        slack_penalty,
    )
    trained_params = rl_agent.rl_trainingloop(
        episode_duration=episode_duration,
        num_episodes=num_episodes,
        replay_buffer=replay_buffer_size,
        episode_updatefreq=episode_update_freq,
        experiment_folder=experiment_folder,
    )
    
    # evaluate the trained policy
    
    stage_cost_after = run_simulation(
        trained_params,
        env,
        experiment_folder,
        episode_duration,
        layers_list,
        after_updates=True,
        horizon=mpc_horizon,
        positions=positions,
        radii=radii,
        modes=modes,
        mode_params=copy.deepcopy(mode_params),
        slack_penalty_eval=slack_penalty,
    )
    
    #save experiment configuration and results
    generate_experiment_notes(
        experiment_folder,
        trained_params,
        params_before,
        episode_duration,
        num_episodes,
        seed,
        alpha,
        dt,
        gamma,
        decay_rate,
        decay_at_end,
        initial_noise_scale,
        noise_variance,
        stage_cost_before,
        stage_cost_after,
        layers_list,
        replay_buffer_size,
        episode_update_freq,
        patience,
        lr_decay,
        mpc_horizon,
        modes,
        copy.deepcopy(mode_params),
        positions,
        radii,
        slack_penalty,
    )

    # append final stage-cost to folder name
    suffix = f"_stagecost_{stage_cost_after:.2f}"
    os.rename(experiment_folder, experiment_folder + suffix)
    
    
if __name__ == "__main__":
    main()
# BASE_DIR = "optuna_runs_1"
# if __name__ == "__main__":
    
#     os.makedirs(BASE_DIR, exist_ok=True)

#     storage_path = os.path.join(BASE_DIR, "optuna.db")
#     storage_uri  = f"sqlite:///{storage_path}"
    
#     # optuna.delete_study(study_name="my_rnn_mpc_study", storage="sqlite:///optuna_runs/optuna.db")
#     study = optuna.create_study(
#         storage=storage_uri,  
#         study_name="rnn_mpc_study",
#         direction="minimize",
#         sampler=optuna.samplers.TPESampler(),
#         pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),
#         load_if_exists=True,   # <-- resume if the DB file already exists
#     )
#     optuna.logging.set_verbosity(optuna.logging.INFO)
    
#     #n_jobs = 1 to run trials sequentially, running in parallel is not supported in this context
#     # matplotlib and casadi are not thread-safe
#     study.optimize(objective, n_trials=50, n_jobs = 1)

#     print("Best trial:",  study.best_trial.number)
#     print("  Value: ",    study.best_value)
#     print("  Params:",    study.best_params)
    
#     save_best_results(study)

